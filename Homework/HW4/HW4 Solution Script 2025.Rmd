---
title: "Homework 4 Solution Script"
author: "Ling Zhu and Songeun Emily Lee"
date: "10/29/2025"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE, error = FALSE,
  prompt = FALSE, comment = "",
  collapse = FALSE,
  tidy = TRUE, tidy.opts = list(width.cutoff = 60) # need to install.packages("formatR")
)
```

# 0. Loading Packages
```{r, warning=FALSE, message=FALSE}
# rm(list=ls()) # Start clean: remove all objects from the workspace
## Libraries
library(foreign)
library(ggplot2)
library(dplyr)
library(ggpubr)
library(VGAM)
library(MASS)
library(GGally)
library(censReg)
library(sampleSelection)
library(tidyr)
library(mvtnorm)
library(AER)        # dispersiontest for Poisson overdispersion
library(MASS)       # glm.nb (Negative Binomial)
library(dplyr)      # data wrangling
library(tidyr)      # pivot_longer
library(ggplot2)    # plotting
library(stargazer)  # model tables

## Working directory
setwd("/Users/songeunlee/Desktop/Ling/HW4")
#setwd("/Users/lingzhu/Dropbox/UH Teaching/POLS6382_2025/HW Assignments/2025 HW Review and Solution Scripts/Homework 4")

## Data
CESdata<-read.csv("CESdata.csv")
```

# 1. Analyzing 2022 Midterm Election Data

## Qeustion 1a and 1b: Data Preparation

```{r}
# ---------------------------------------------------------------
# Question 1a:
# Download the 2022 CES post-election data from https://cces.gov.harvard.edu/
# This code cleans and recodes key variables for analysis.
# ---------------------------------------------------------------

# Load packages
library(dplyr)
library(tidyr)

# Remove missing cases only for the variables we need
CESdata <- CESdata %>%
  drop_na(race, birthyear, gender, party, votereg, vote, education)

# Recode variables
CESdata <- CESdata %>%
  mutate(
    white     = ifelse(race == 1, 1, 0),          # 1 = White, 0 = Non-White
    age       = 2022 - birthyear,                 # Age = 2022 minus birth year
    male      = ifelse(gender == 1, 1, 0),        # 1 = Male, 0 = Female/Other
    Democrat  = ifelse(party == 1, 1, 0),         # 1 = Democrat, 0 = Others
    votereg   = ifelse(votereg == 1, 1, 0),       # 1 = Registered, 0 = Not/Don't know
    vote      = ifelse(vote == 5, 1, 0),          # 1 = Voted, 0 = Did not vote
    college   = ifelse(education %in% c(3,4,5,6), 1, 0)  # 1 = Some college or more
  )

# Keep only the variables needed for this exercise
data1 <- CESdata %>%
  dplyr::select(white, age, male, Democrat, votereg, vote, college)

# Check data summary
summary(data1) # Check the dataset with View(data1) or colnames(data1)

# Save the cleaned dataset
write.csv(data1, "ces2022_clean.csv", row.names = FALSE)

```

## Question 1c: Estimating a Standard Logit

```{r}
# ---------------------------------------------------------------
# Question 1c: Predicting the Probability of Voting
# Estimate standard logit models using 2022 CES data
# ---------------------------------------------------------------

library(dplyr)
library(ggplot2)

# --- Model 1: Predict voting using demographics and partisanship ---
logit_model1 <- glm(vote ~ white + age + male + Democrat + college,
                    data = data1, family = binomial)

summary(logit_model1)          # Model summary (log-odds)
exp(coef(logit_model1))        # Exponentiated coefficients (odds ratios)

# --- Model 2: Add voter registration as an additional predictor ---
logit_model2 <- glm(vote ~ white + age + male + Democrat + college + votereg,
                    data = data1, family = binomial)

summary(logit_model2)
exp(coef(logit_model2))

# ---------------------------------------------------------------
# Plotting Predicted Probabilities (Model 1)
# ---------------------------------------------------------------

# Calculate variable means for continuous and binary predictors
mean_white    <- mean(data1$white)
mean_age      <- mean(data1$age)
mean_male     <- mean(data1$male)
mean_Democrat <- mean(data1$Democrat)

# Create a new dataset varying college (0 = No College, 1 = College)
new_data <- data.frame(
  white     = mean_white,
  age       = mean_age,
  male      = mean_male,
  Democrat  = mean_Democrat,
  college   = c(0, 1)
)

# Predict log-odds and convert to probabilities with 95% CI
pred <- predict(logit_model1, newdata = new_data, type = "link", se.fit = TRUE)
prob <- plogis(pred$fit)
lower <- plogis(pred$fit - 1.96 * pred$se.fit)
upper <- plogis(pred$fit + 1.96 * pred$se.fit)

plot_data <- data.frame(
  college = factor(new_data$college, labels = c("No College", "College")),
  prob = prob, lower = lower, upper = upper
)

# --- Plot predicted probabilities with confidence intervals ---
ggplot(plot_data, aes(x = college, y = prob)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) +
  labs(x = "Education", y = "Predicted Probability of Voting") +
  ylim(0, 1) +
  theme_minimal()

```

```{r}
library(ggeffects)
pdturnout <- ggpredict(logit_model1, terms = "college [0,1]")

# make clean labels in the order you want
pdturnout$x <- factor(pdturnout$x,
                      levels = c(0, 1),
                      labels = c("No College", "College or Above"))

library(ggplot2)
ggplot(pdturnout, aes(x = x, y = predicted)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high),
                width = 0.1, color = "royalblue") +
  labs(x = "Education Attainment",
       y = "Predicted Probability of Voting") +
  coord_cartesian(ylim = c(0, 1)) +  # optional: clamp to [0,1]
  theme_bw() +
  theme(
    legend.position = "none",
    axis.title = element_text(size = 14)
  )

```

## Question 1d: Estimating a Two-Step Heckman Selection Model

```{r}
# Load the package for Heckman selection models
library(sampleSelection)

# Estimate the Heckman two-step selection model
selectionmodel1 <- selection(
  selection = votereg ~ age + college,
  outcome   = vote ~ white + age + male + Democrat + college,
  data = data1,
  method = "2step"
)

# Display the model summary
summary(selectionmodel1)

```



```{r}
# install.packages("GJRM")
library(GJRM)

# Ensure both dependent variables are coded as 0/1 integers (Bernoulli responses expected by probit links)
data1 <- transform(
  data1,
  votereg = as.integer(votereg %in% 1),  # selection indicator: 1 = registered, 0 = not
  vote    = as.integer(vote %in% 1)      # outcome: 1 = voted, 0 = did not vote
)

# Specify the two equations:
# - eq_sel: selection (who is “in sample” for the outcome process—registered to vote)
# - eq_out: outcome (who votes among those eligible/selected)
eq_sel <- votereg ~ age + college
eq_out <- vote    ~ white + age + male + Democrat + college

# Fit a Bivariate Sample Selection (BSS) model:
# - FIML joint estimation of two Bernoulli–probit equations
# - Gaussian copula ties the latent errors, estimating their correlation (theta = rho)
# - margins = c("probit","probit") sets probit links for both equations
fit <- gjrm(
  list(eq_sel, eq_out),
  data    = data1,
  model   = "BSS",                 # bivariate probit selection model (FIML)
  margins = c("probit","probit")   # both equations are probit
)

# Summarize results:
# - Equation 1: probit coefficients for registration (selection)
# - Equation 2: probit coefficients for voting (outcome, conditional on selection)
# - theta: dependence parameter (latent error correlation = rho); tests endogenous selection
# - n / n.sel: total observations / number effectively in the selected regime
summary(fit)

```

## Question 1e: Comparing Specifications

```{r}
summary(logit_model1)
summary(selectionmodel1)
```

# 2. Analyzing Count Data

## Question 2a and 2b: Estimating a Poisson Model

```{r}
# Load the dataset (contains data on presidential vetoes)
load("vetos.RData")

# Load stargazer for formatted regression output tables
require(stargazer)

# (1) Poisson Regression Model (Full Specification)
# Dependent variable: nover (number of vetoes)
# Includes multiple predictors: congressional experience, governor experience, presidential party,
# House majority, Senate majority, and presidential election proximity
poissonmod <- glm(
  nover ~ congexpr + govexpr + prespty + hmajor + smajor + presepct,
  data = vetos,
  family = "poisson"    # Poisson regression for count data
)

# (2) Poisson Regression Model (Reduced Specification)
# A more parsimonious model including only key predictors
poissonmod2 <- glm(
  nover ~ congexpr + govexpr + prespty,
  data = vetos,
  family = "poisson"
)

# Display both models side-by-side in a formatted table
# Title indicates comparison between a full and reduced Poisson model
stargazer(
  poissonmod, poissonmod2,
  type = "text",
  title = "Poisson Regression Models Predicting the Number of Vetoes"
)

```

## Question 2C: Methods of Substantively Interpreting Results

```{r}
# --- Incidence Rate Ratios (IRRs) with 95% CIs ------------------------------
# Do NOT overwrite the base 'coef' function name; store into 'coef_ci' instead.
# 'confint(poissonmod)' gives profile-likelihood CIs on the *log* scale.
# Exponentiating turns log-coefficients into IRRs and CIs onto the IRR scale.
coef_ci <- cbind(Estimate = coef(poissonmod),
                 confint(poissonmod))  # may profile; can be slow on big models
IRR <- exp(coef_ci)
IRR

# --- Predicted counts across 'presepct' with 95% CI (on response scale) -----
# IMPORTANT: For GLMs, standard errors are returned on the *link* scale.
# So: predict on 'link', build CIs on link, then transform via inverse link (exp)
# to the response (mean count) scale.

# Build a prediction grid: vary 'presepct' 0..100, hold others at their medians
newdata <- data.frame(
  presepct = seq(0, 100, length = 30),
  govexpr  = median(vetos$govexpr, na.rm = TRUE),
  prespty  = median(vetos$prespty, na.rm = TRUE),
  congexpr = median(vetos$congexpr, na.rm = TRUE),
  hmajor   = median(vetos$hmajor, na.rm = TRUE),
  smajor   = median(vetos$smajor, na.rm = TRUE)
)

# Predict on the LINK scale with SEs
pred_link <- predict(poissonmod, newdata, type = "link", se.fit = TRUE)

# Transform to RESPONSE scale (mean counts) and build 95% CIs correctly
predicdata <- within(newdata, {
  eta  = pred_link$fit                         # linear predictor
  se   = pred_link$se.fit                      # its standard error
  LL   = exp(eta - 1.96 * se)                  # lower CI on response scale
  UL   = exp(eta + 1.96 * se)                  # upper CI on response scale
  fit  = exp(eta)                              # mean predicted counts
})

# --- Plot: predicted mean counts vs. 'presepct' with 95% CI errorbars -------
# Points + line for the mean, error bars for 95% CI, clean theme
library(ggplot2)

ggplot(predicdata, aes(x = presepct, y = fit)) +
  geom_point() +
  geom_line(size = 0.8) +
  geom_errorbar(aes(ymin = LL, ymax = UL), width = 1.5, size = 0.5, color = "gray50") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  labs(x = "President Electoral College Percent",
       y = "Predicted Number of Vetoes Cast",
       title = "Poisson GLM Predictions with 95% Confidence Intervals")

```

## Question 2D: Testing for Overdispersion and Estimating a Negative Binomial Model

```{r}
 # --- Fit competing count models ----------------------------------------------
# Poisson regression (PRM)
poissonmod <- glm(
  nover ~ congexpr + govexpr + prespty + hmajor + smajor + presepct,
  data   = vetos,
  family = poisson(link = "log")
)

# Overdispersion diagnostic for Poisson (H0: equidispersion)
dispersiontest(poissonmod)

# Negative Binomial regression (accounts for overdispersion via theta)
negbinomod <- glm.nb(
  nover ~ congexpr + govexpr + prespty + hmajor + smajor + presepct,
  data = vetos
)

# Likelihood-based comparison (NB nests Poisson when theta)
anova(poissonmod, negbinomod, test = "Chisq")

# --- Side-by-side model table -------------------------------------------------
stargazer(
  poissonmod, negbinomod,
  type  = "text",
  title = "Comparison of Poisson vs. Negative Binomial Regression for Veto Counts",
  dep.var.labels = "Number of Vetoes (nover)",
  covariate.labels = c("Congressional Experience",
                       "Governor Experience",
                       "President's Party (Indicator)",
                       "House Majority (President's Party)",
                       "Senate Majority (President's Party)",
                       "Pres. Electoral College %"),
  digits = 3
)

# --- Predicted values: Poisson vs. NB (Simple Version) -----------------------
# Create a simple data frame with observed and predicted counts
compare <- data.frame(
  id            = seq_len(nrow(vetos)),
  nover         = vetos$nover,
  prmpredicted  = round(fitted(poissonmod), 1),
  nbrmpredicted = round(fitted(negbinomod), 1)
)

# Convert to long format for ggplot (base R reshape)
compare_long <- reshape(
  compare,
  varying = c("nover", "prmpredicted", "nbrmpredicted"),
  v.names = "Counts",
  timevar = "model",
  times = c("Observed", "Poisson Predicted", "NB Predicted"),
  direction = "long"
)
rownames(compare_long) <- NULL

# Make model a factor to control legend order/labels
compare_long$model <- factor(
  compare_long$model,
  levels = c("Observed", "Poisson Predicted", "NB Predicted")
)

# --- Plot observed vs. fitted (PRM vs. NB) -----------------------------------
ggplot(compare_long, aes(x = id, y = Counts, color = model, group = model, shape = model)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_x_continuous(breaks = seq(1, max(compare_long$id), 2)) +
  labs(x = "Observation Index", y = "Counts",
       title = "Observed vs. Predicted Counts: Poisson vs. Negative Binomial") +
  theme_bw() +
  theme(
    panel.grid        = element_blank(),
    legend.position   = "bottom",
    legend.background = element_rect(color = "grey80"),
    legend.title      = element_blank(),
    legend.text       = element_text(size = 12),
    axis.text         = element_text(size = 12),
    axis.title        = element_text(size = 12)
  )
```


