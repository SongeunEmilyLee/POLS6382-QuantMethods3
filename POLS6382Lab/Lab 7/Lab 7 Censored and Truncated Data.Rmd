---
title: '\Large \textbf{POLS6382 Quantitative Method III\\ Maximum Likelihood Estimation}'
subtitle: '\large \textbf{Lab 7: Censored and Trancated Data}'
author: 
  - Ling Zhu and Emily Lee
  - Department of Political Science
  - University of Houston
date: "2025/10/22"
output: pdf_document
header-includes: 
  - \renewcommand{\and}{\\}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, error=FALSE, warning=FALSE,
                      prompt=TRUE, comment='', collapse=FALSE)
```

# 1. \normalsize Learning Objectives
\section*{\normalsize Objectives}
\begin{itemize} 
\item Learn how to estimate a Tobit regression model.
\item Compare Tobit and OLS results with censored data. 
\item Learn how to estimate a Heckman selection model. 
\item Assessing bias due to sample selection.  
\end{itemize}

```{r message=FALSE}
rm(list=ls())
setwd("/Users/songeunlee/Documents/GitHub/POLS6382-QuantMethods3/POLS6382Lab/Lab 7")
my_packages <- c("foreign", "ggplot2","dplyr","ggpubr","VGAM","MASS",
                 "GGally","censReg", "sampleSelection", "tidyr","mvtnorm") 
invisible(lapply(my_packages, require, character.only = TRUE))
```


# 2. \normalsize Compare Tobit and OLS with Censored Data

In this section, we will use a simulated data et to compare the Tobit and OLS model. First, we simulate a dataset that contains a dependent variable y, which is left-censored. All the censored cases are assigned with value ``0". The explanatory variable is x, generated by randomly drawing numbers from a normal distribution. We also set the latent variable y.star as a linear function of x, with a slope coefficient of 5.

```{r}
N = 10
f = rep(c("s1","s2","s3","s4","s5","s6","s7","s8"),N)
fcoeff = rep(c(-1,-2,-3,-4,-3,-5,-10,-5),N)
set.seed(100) 
x = rnorm(20*N)+1
beta = 5
epsilon = rnorm(20*N,sd = sqrt(1/5))
y.star = x*beta+fcoeff+epsilon ## latent response
y = y.star 
y[y<0] <- 0 ## left censored response
simdata<-data.frame(cbind(x,y))
```

Next, we fit an OLS model with this simulated dataset. We see that the OLS model produces a slope coefficient of 2.93, which is substantially smaller than the true parameter value that we set ($\beta$=5). The biased coefficient is caused by ignoring the fact that values of y are censored at 0. 

The proper model specification is Tobit. We use the \texttt{vglm()} function to estimate a Tobit model.\footnote{Various \texttt{R} functions can be used to estimate a Tobit model, e.g. \texttt{censReg()} from package \texttt{censReg} and \texttt{tobit()} from package \texttt{AER}.}. The first coefficient labeled as ``(intercept):1" is the intercept term. The second coefficient labeled as ``(intercept):2" an ancillary statistic. If we exponentiate this value, we get a number that is analogous to the square root of the residual variance in OLS regression. $exp(0.718)\approx 2.05$. We also see that the Tobit model produces a mean slope of 4.856 for variable x. This is quite close to the true parameter value (5). 

```{r}
fitols<-lm(y~x)
summary(fitols)
fittobit<-vglm(formula=y~x, family=tobit(Lower=0))
summary(fittobit)
coef(fitols)
coef(fittobit) # More satisfying estimates
```

We can also compare the two models by plotting the observations and the two regression lines. Figure 1 shows the Tobit model (orange line) and the OLS model (red line) produces very different slope coefficients. Ignoring that $y$ is censored, the OLS model underestimates the slope coefficient of x. 

```{r fig.width=5, fig.height=5}
# Compare two regression lines
plot(x,y)
abline(lm(y~x),col="red",lwd=2,lty=1)
curve(-3.83444  + 4.85 *x, col="orange", lwd="2", add=TRUE)
```


# 3. \normalsize Tobit Model for Left- and Right-Censored Data
We use a dataset named \texttt{tobit.csv}. This dataset considers the situation in which we have a measure of academic aptitude (scaled 200-800), which we want to model using reading and math test scores, as well as the type of program the student is enrolled in (academic, general, or vocational). The problem here is that students who answered all questions on the academic aptitude test correctly receive a score of 800, even though it is likely that these students are not ``truly" equal in aptitude. The same is true of students who answer all of the questions incorrectly. All such students would score 200, although they may not all be of equal aptitude.


The dataset contains 200 observations. The academic aptitude variable is \texttt{apt}, the reading and math test scores are \texttt{read} and \texttt{math} respectively. The variable \texttt{prog} is the type of program the student is in, it is a categorical (nominal) variable that takes on three values:academic (\texttt{prog} = 1), general (\texttt{prog} = 2), and vocational (\texttt{prog} = 3). The variable \texttt{id} indexes different students.

Let us start from looking at the dependent variable \texttt{apt} descriptively. Note that in this dataset, the lowest value of \texttt{apt} is 352. That is, no students received a score of 200 (the lowest score possible), meaning that even though censoring at the bottom was possible, it does not occur in the data set.

The following figure is a histogram plot, showing the frequency of different scores. It clearly shows that the censoring in the values of \texttt{apt}. There are far more cases with scores of 750 to 800 than one would expect looking at the rest of the distribution. 

```{r fig.width=5, fig.height=5}
mydata<-read.csv("tobit.csv")
attach(mydata)
summary(mydata$apt)

f <- function(x, var, bw = 15) {
  dnorm(x, mean = mean(var), sd(var)) * length(var)  * bw
}

ggplot(mydata, aes(x = apt, fill=prog))+
  stat_bin(binwidth=15) + 
  stat_function(fun = f, size = 1,args = list(var = mydata$apt))+
  scale_fill_brewer(palette = "Blues",
                       name = "Program:",
                       labels = c("Academic",
                                  "General",
                                  "Vocational"))+
  theme_light()+
  theme(legend.position = "bottom",
        axis.title = element_text(size = 14))
```

We can also describe variables by using a correlation matrix figure, showing the pair-wise correlations between reading score, math score and the apt scale. 

```{r fig.width=5, fig.height=5}
cor(mydata[, c("read", "math", "apt")])
ggpairs(mydata[, c("read", "math", "apt")])+
  theme_light()
```

Because the data are top-censored, we can we run a Tobit model, using the \texttt{vglm} function from the VGAM package. 

```{r}
tobitmodel<-vglm(formula=apt~read+math+as.factor(prog), 
                 family=tobit(Upper=800))
summary(tobitmodel)
```
Based on this model, we can interpret the results substantively as the following:
\begin{itemize}
\item For a one unit increase in \texttt{read}, there is a 2.6981 point increase in the predicted value of \texttt{apt}.
\item A one unit increase in \texttt{math} is associated with a 5.9146 unit increase in the predicted value of \texttt{apt}.
\item The terms for \texttt{prog} have a slightly different interpretation. The predicted value of \texttt{apt} is 46.1419 points lower for students in a vocational program than for students in an academic program.
\item We do not observe statistically different predicted \texttt{apt} scores between students in a general program and those in an academic program.
\end{itemize}


We can test the significance of program type overall by fitting a model without the variable `` program",  and using the likelihood ratio test to compare two models.  The LRT with two degrees of freedom is associated with a p-value of 0.0032, indicating that the overall effect of \texttt{prog} is statistically significant.

```{r}
tobitmodel2<-vglm(apt ~ read + math, tobit(Upper = 800), 
                  data = mydata)
(p <- pchisq(2 * (logLik(tobitmodel) - logLik(tobitmodel2)), 
             df = 2, lower.tail = FALSE))
```


# 4. \normalsize Heckman Sample Selection Models: Simulated Data

In this section, we use a simulated dataset to show how we can estimate a correctly specified the Heckman selection model with exclusion restriction. We simulate the data by the following steps.
\begin{itemize}
\item Using \texttt{mvtnorm}, we create bivariate normal disturbances with correlation -0.7. This is the correlation parameter $\rho$ between our selection and outcome equation.
\item We generate a uniformly distributed explanatory variable for the selection equation, \texttt{xs}, the selection outcome \texttt{ys} by Probit data generating process,
\item The explanatory variable for the outcome equation \texttt{xo} is also drawn from a uniform distribution.
\item All our true intercepts are equal to 0 and our true slopes are equal to 1, both in this and the following examples.
\item  The latent outcome variable is \texttt{yoX}, and the observable outcome is \texttt{yo}. Note that the vectors of explanatory variables for the selection (xs) and outcome equation (xo) are independent and hence the exclusion restriction is fulfilled. 
\end{itemize}

```{r}
set.seed(0)
eps <- rmvnorm(500, c(0, 0), 
     matrix(c(1, -0.7, -0.7, 1), 2, 2))
# selection
xs <- runif(500)
ys <- xs + eps[, 1] > 0
# outcome
xo <- runif(500)
yoX <- xo + eps[, 2]
yo <- yoX * (ys > 0)

```
Next, we run a Heckman selection model using function \texttt{selection} from \texttt{sampleSelection} package. The first model component is the selection equation, and the second component is the outcome equation. We see that the estimates are reasonably precise.

```{r}
summary(selection(ys ~ xs, yo ~ xs))

```

Now we repeat the same exercise, but without the exclusion restriction, generating \texttt{yo} using \texttt{xs} instead of \texttt{xo}. The estimates are still unbiased but standard errors are substantially larger in this case. The exclusion restriction--independent information about the selection process--has a certain identifying power that we now have lost. We are solely relying on the functional form identification.

```{r}
yoX <- xs + eps[, 2]
yo <- yoX * (ys > 0)
summary(selection(ys ~ xs, yo ~ xs))

```


# 5. \normalsize Heckman Selection Model with Observational Data

The dataset used in this example is included in \texttt{sampleSelection}, named \texttt{Mroz87}. This dataset was used by Mroz (1987) for analyzing female labor supply. In this example, labor force participation (described by dummy \texttt{lfp}) is modeled by a quadratic polynomial in age (\texttt{age}), family income (\texttt{faminc}, in 1975 dollars), presence of children (\texttt{kids}), and education in years (\texttt{educ}). The wage equation includes a quadratic polynomial in experience (\texttt{exper}), education in years (\texttt{educ}), and residence in a big city (\texttt{city}). First, we estimate the model by the Heckman two-step method.


```{r}
data("Mroz87")
Mroz87$kids <- (Mroz87$kids5 + Mroz87$kids618 > 0)
selectmod1 <- selection(lfp ~ age + I(age^2) + faminc + kids + educ,
  +   wage ~ exper + I(exper^2) + educ + city, data = Mroz87, 
    method = "2step")
summary(selectmod1)
```

In this exercise, we are interested in examining the determinants of female workers' wages. To do so, we have to consider the selection process that determines female labor participation. The above model shows that education affects both labor participation and wages.  Notice that the above model also produces ``NAs" for sigma and $\rho$, meaning we have a nonpositive and definite variance-covariance matrix. This could be because we add quadratic polynomial terms in both equations. When this issue occurs, we can consider the ML estimation. 

```{r}
selectmod2 <- selection(lfp ~ age + I(age^2) + faminc + kids + educ,
          +wage ~ exper + I(exper^2) + educ + city, data = Mroz87,
          maxMethod = "BHHH", iterlim = 500) 
summary(selectmod2)
```
