---
title: '\Large \textbf{POLS6382 Quantitative Method III\\ Maximum Likelihood Estimation}'
subtitle: '\large \textbf{Lab 4: Binary Choice Models (2)}'
author: 
  - Ling Zhu and Emily Lee
  - Department of Political Science
  - University of Houston
date: "2025/10/01"
output: pdf_document
header-includes: 
  - \renewcommand{\and}{\\}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, error=FALSE, warning=FALSE,
                      prompt=TRUE, comment='', collapse=FALSE)
```

# 1. \normalsize Learning Objectives
\begin{itemize} 
\item Learn how to estimate a heteroskedastic probit model.
\item Learn how to estimate a rare-event logit model.
\item Learn how to use post-estimation simulation to consider uncertainty (of parameter estimation) in prediction calculations.
\end{itemize}

```{r message=FALSE}
rm(list=ls())
setwd("/Users/songeunlee/Desktop/Lab 4") # setwd("/Users/lingzhu/Dropbox/UH Teaching/POLS6382_2025/2025 Labs/Lab 4")
my_packages <- c("AER", "ggplot2", "glmx", "heatmapFit",
                 "lmtest", "maxLik", "reshape2", "Zelig", "MASS","stats") 
invisible(lapply(my_packages, require, character.only = TRUE)) 
```


# 2. \normalsize Comparing Standard and Heteroskedastic Probit Model with Simulated Data
## 2.1  \normalsize Describing Variables
When heteroskedasticity occurs,  a standard Probit model will produce inconsistent and inefficient estimation, thus we need to consider a heteroskedastic Probit model as the alternative. The function \texttt{hetglm()}from package \texttt{glmx} can be used to fit a heteroskedastic Probit model. In this section, we'll use a simulated data set to show why a standard Probit model is problematic when the homoskedasticity assumption is violated. We'll also compare estimation results between a standard and a heteroskedastic Probit model. 

The data simulation process is specified as the following:
\begin{enumerate}
\item Define sample size: n=500.
\item Randomly draw 500 observations for variable $y$ from a standard normal distribution.
\item Define the latent continuous scale of $ystar$, as such, $ystar=1+x+\epsilon$. $\epsilon$ is drawn from a normal distribution with a mean of 0. The standard deviation is set to be $exp(x)$. Defining $sd=exp(x)$, we let the variance of $\epsilon$ vary along x. Hence, $\epsilon^2$ is not constant. 
\item Simulate observed binary variable $y$ based on the latent scale $ystar$. \texttt{if else()} is used to define the classification rule: classify the observation to be 1 if$ystar >0$, and 0 otherwise. 
\item Make a data frame by combining the variables y and x. 
\end{enumerate}


```{r message=FALSE}
set.seed(48)
n<- 500
x<- rnorm(n)
ystar<-1+x+rnorm(n, sd = exp(x))
y<-ifelse(ystar>0, 1, 0)
simdata<-data.frame(cbind(y,x))
```

We can visualize our simulated data, showing both the dependent variable (y) and the association between x and y. Visualizing the simulated data, we see that $y^*$ has non-constant error variance. By our simulation, we classify 350 cases with 1s, and 150 cases with 0s. 

```{r message=FALSE, fig.width=7, fig.height=3}
visual1<-ggplot(data=simdata)+
  geom_point(aes(x=x, y=ystar))+
  labs(x="Simulated X (n=500)",
       y="Latent Values of Y")+
  theme_light()
  
visual2<-ggplot(data=simdata)+
  geom_point(aes(x=x, y=y))+
  labs(x="Simulated X (n=500)",
       y="Simulated Outcomes")+
  theme_light()
  
visual3<-ggplot(data=simdata)+
  geom_histogram(aes(x=y), binwidth = 0.2, color="black", fill="gray")+
  labs(x="Simulated Binary Y",
       y="Count")+
  theme_light()
  
require(ggpubr)
ggarrange(visual1,visual2, visual3, ncol=3, nrow=1)
```


## 2.2  \normalsize Estimating a Probit Model When Error Variance Is Not Constant
What would happen if we estimate a standard probit model when the variance is not constant? It will produce biased results. Model 1, shown below, reports two coefficients: intercept=0.521, the slope coefficient of $x$ is 0.345. Both numbers deviate from 1 (the true values) substantially (i.e., biased). 
```{r message=FALSE}
model1 <- glm(y ~ x, family = binomial(link = "probit"),data=simdata)
summary(model1)
```

## 2.3 \normalsize Heteroskedastic Probit Model
A more appropriate specification is the heteroskedastic Probit model, using which we can specify a scale parameter to model the non-constant variance term. The syntax of \texttt{hetglm()} is as the following. We define the variance as a linear function of x (recall that we set the variance to have a mean of 0, but sd is defined as a function of x). 

```{r}
model2 <- hetglm(y ~ x | x, family=binomial(link="probit"),data=simdata)
summary(model2)
```

We observe that the heteroskedastic Probit model produces coefficients that are much closer to the true intercept (1) and true slope (1). The estimated latent scale variance parameter has a positive coefficient. This means that the variance of $y^*$ increases as $x$ increases, which is just as what we defined. The following figure compares the two specifications using heatmap fit plot. Figure 1-(1) clearly shows that Model 1 is a mis-specified model. Model 2 substantially improves the model fit. 

```{r, fig.height=4, fig.width=4}
y<-simdata$y
pred1<-predict(model1,type="response")
pred2<-predict(model2,type="response")
heatmap.fit(y,pred1,reps=1000,legend=FALSE)
heatmap.fit(y,pred2,reps=1000,legend=FALSE)
```

# 3. \normalsize Estimate a Heteroskedastic Probit Model with Real Data
In this section, we use a real data example to compare standard and heteroskedastic Probit model. We use the labor force participation data example from package\texttt{AER}. This is also an empirical example discussed in Scott Long's book chapter. The dataset contains 753 observations and 21 variables. The variable of interest is a binary variable \texttt{participation}, which is coded as 1 if an individual is in labor force, and 0 otherwise. 

```{r}
load("/Users/songeunlee/Desktop/Lab 4/PSID1976.rda") # load("PSID1976.rda")
#data("PSID1976", package = "AER")
PSID1976$kids <- with(PSID1976, factor((youngkids + oldkids) > 0,
                levels = c(FALSE, TRUE), labels = c("no", "yes")))
PSID1976$fincome <- PSID1976$fincome/10000

#Probit model using glm()
labormodel1<- glm(participation ~ age + I(age^2) + 
              fincome + education + kids,
              data = PSID1976, family = binomial(link = "probit"))
summary(labormodel1)

# Standard probit model via hetglm() with constant scale
labormodel2 <- hetglm(participation ~ age + I(age^2) + 
                fincome + education + kids | 1,
                data = PSID1976)
summary(labormodel2)

#Heteroskedastic Probit model with varying scale
heterlabor<- hetglm(participation ~ age + I(age^2) + 
              fincome + education + kids | kids + fincome,
               data = PSID1976)
summary(heterlabor)
```





The first model equation is a standard probit model estimated using glm(). The second model equation estimates the same standard probit using \texttt{heterglm()}, by setting the latent scale to be 1. By doing so, we will have the same number of parameters for this model and the heteroskedastic probit model (heterlabor). We can perform the likelihood ratio test to compare two models. 

```{r}
lrtest(labormodel2, heterlabor)
```

# 4. \normalsize Post Estimation Simulations with Zelig
## 4.1 \normalsize What Is Zelig and What Does Zelig Do?
\texttt{Zelig} is an R package developed by Kosuke Imai, Gary King, and Olivia Lau. It is a common framework for estimating a variety of statistical models and integrates simulation-based methods (e.g. bootstrapping, Bayeisan inference, etc.) to improve model performance and the accuracy of predictions. A similar (but less powerful) \texttt{Stata} module developed by Gary King and his associates is \texttt{Clarify}. For more detailed discussions on \texttt{Zelig} and \texttt{Clarify}, see the following two articles.
\begin{enumerate}
\item King, Gary, Michael Tomz, and Jason Wittenberg. 2000. ``Making the Most of Statistical Analyses: Improving Interpretation and Presentation." \textit{American Journal of Political Science} 44(2):347-361.
\item Kosuke Imai, Gary King, and Olivia Lau. ``Toward A Common Framework for Statistical Analysis and Development." \textit{Journal of Computational and Graphical Statistics} 17(4): 892-913.
\end{enumerate}
\texttt{Zelig} can do a variety of things: simulating scientific quantities of interest, performing point and uncertainty estimation, multiple imputation, matching methods, counterfactual evaluations, replication, etc. It also supports a large number of statistical models: Bayesian and frequentist models, multiple equation models, times-series-cross-section models, multi-level models, etc. 

## 4.2 \normalsize Point/Uncertainty Estimates and Probability Calculation
In this section, we use the \texttt{turnout} example from \texttt{Zelig} to show how one can use \texttt{Zelig} to estimate a binary response model, then use simulation-based methods to perform out-sample predictions. This is also one of the examples used in King et. al AJPS 2000 paper regarding \texttt{Clarify}.

Road map:
\begin{enumerate}
\item Estimate the model using function \texttt{zelig()}.
\item Sett specific values for explanatory variables using function \texttt{setx()}.
\item Compute quantity of interest using function \texttt{sim()}. \texttt{Zelig} supports simulations for expected values given the model and $x$, predicted values given by the fitted values, first differences, risk ratios, average predicted and expected treatment effects.
\end{enumerate}

We specify the probability of turnout as a function of individuals' race, education, age, and income. In the model, we also add a square term of $age$ to depict the curvy linear relationship between age and the probability of turnout.We find that both age and education significantly affect the probability of turnout. After estimating the logit model, we are interested in calculating the predicted probabilities of $turnout$ along $age$ and $education$. We do so by setting the values of $age$ and $education$.The first statement in \texttt{sets()} is the model object. The second statement, ``\texttt{education=12}" sets the year of education to be 12. ``\texttt{age=18:95}" let values for $age$ change across its full range. Note that we did not specify the values for $income$ and $age^2$. \texttt{setx()} will automatically calculate $age^2$ based on the values we set for $age$. All remaining variables are held at their means as the default. $setx()$ also defines the intercept to be 1 as the default. 

```{r, fig.height=4,fig.width=4}
# Zelig is no longer available on CRAN. 
# To install it in R, you need to use GitHub:
install.packages("remotes")   # Install 'remotes' if not already installed
library(remotes)              # Load the 'remotes' package
install_github("IQSS/Zelig")  # Install Zelig from GitHub
library(Zelig)                # Load the Zelig package

data(turnout,package="Zelig")
# Estimate the model
z.out<-zelig(vote~race+educate+age+I(age^2)+income,
             model="logit",data=turnout)
# Set explanatory variables 
x.low<-setx(z.out, educate=12, age=18:95)
x.high<-setx(z.out, educate=16, age=18:95)
#Simulate quantities of interest:
s.out<-sim(z.out, x=x.low, x1=x.high)
plot(s.out, xlab="Age in Years",
        ylab="Predited Probability of Voting",
        xlim=c(20,90))
```

# 5. \normalsize Analyzing Rare Events Data with Zelig
\texttt{Zelig} also encompasses the rare-event logit regression model discussed in King and Zeng's two papers in \textit{Political Analysis} and \textit{International Organization}, respectively. Using \texttt{Zelig} to estimate a rare-event logit model is quite simple. In this section, we use the \texttt{mid} data example from \texttt{Zelig} to show the estimation process. 

In this data file, King et al. designed a 1:2 sample, including all cases of conflict, and about 2,000 cases of non-conflict. We get comparable results to what King and Zeng report in the Table 1 of their $IO$ article. In our specification, we use the original sample proportion to define the value of $\tau$. The method of bias correction is defined as ``prior" (prior correction). If we write ``weighting", then \texttt{Zelig} with just use the weighting method for bias correction. Similarly, we can use \texttt{setx()} and \texttt{sim()} to obtain/present the quantity of interest in a figure. Figure 3 is an example of how the probability of conflict changes as the value of $power$ changes from 0 to 1. Appendix 1 includes R code to create different a cased-controlled sample (1:1)
.
```{r, fig.height=4, fig.width=4}
data(mid,package="Zelig")
reventmod<-zelig(conflict~major+ contig+power+maxdem+mindem+years, data=mid,
                model="relogit",tau=1042/303772, 
                case.control="prior", bias.correct=TRUE)
# summarize the model output
summary(reventmod)
# Let variable ``power" vary, and hold all other variables constant.
x.out1<-setx(reventmod,power=0:1)
#Simulate quantity of interests
s.out1<-sim(reventmod,x=x.out1)
plot(s.out1,ci=95)

```

# 6.\normalsize Appendix 1: Code to Create a 1:1 Sample
```{r}
data(mid,package="Zelig")
# create a subset only for cases if conflict=1
subset1<-mid[which(mid$conflict=='1'),]
# create a subset only for cases if conflict=0
subset2<-mid[which(mid$conflict=='0'),]
# creat a sample from subset2, randomly draw 1042 observations
sample1<-subset2[sample(1:nrow(subset2),1042),]
# create a new data frame, with a 1:1 sample design for 1s and 0s
newdata<-rbind(subset1, sample1)
```
# 7. \normalsize Appendix 2. More on simulations: visually weighted logit regression lines
In Appendix 2, we will use another simulated data example to further illustrate how post-estimation simulations can help us gauge uncertainty in coefficients. First, we simulate a data set, in which we have 1,000 observations. In this data set, we include two explanatory variables: $X$, coded as continuous and distributed as a standard normal distribution, and $Female$, coded as dichotomous. We define the latent continuous scale of $y$ as $2X+10Female+\epsilon$. We assume that the variance of $\epsilon$ is constant and normally distributed. The observed binary outcomes of $y$ are simulated as 1s and 0s in this data example. 

The estimated model shows our basic specification, that the probability of observing $Y=1$ is positively associated with both $X$ and $Gender$. This can be generalized to any empirical examples, whereby we theorize that the binary dependent variable $Y$ is predicted by a continuous variable $X$ and dummy variable $Z$. When we concern about uncertainty in our estimated coefficients, we can use post-estimation simulations to improve the probability calculation. We do so by the following steps.
```{r}
#Simulate Some Data
n <- 1000
simdata2<- data.frame(X = rnorm(n),
            Female = sample(c(0, 1), n, replace = T))
simdata2$Y <- with(simdata2, X * 2 + Female * 10 + rnorm(n, sd = 5))
simdata2$Y <- (simdata2$Y > 1) * 1
head(simdata2)

# Model:
myModel <- glm(Y ~ X + Female, data = simdata2, family = "binomial")
```
 
\begin{enumerate}
\item Define the number of simulations and some quantity of $X$ and $Female$ for out-sample prediction. Here, we want 1,000 simulations. We also set the value range of $X$ to be between the min and max values observed in our dataset. For variable $Female$, we just pick the two scenarios: $Female=0$ and $Female=1$. 

\item Randomly generate 1,000 draws of the parameter values for $\beta_{X}$ and $\beta_{Female}$, for both the coefficients and variance. Because now we have more than one explanatory variables, instead of drawing values from a normal distribution, we need to get  the 1,000 draws from a multivariate normal distribution. Note that by doing this simulation, we produce two sample distributions for our estimated coefficients  $\beta_{X}$ and $\beta_{Female}$, and two sample distributions for their associated variance  $\sigma_{X}^2$ and $\sigma_{Female}^2$. In each of these sample distributions, we have 1,000 cases. After the simulation, we calculate predicted probabilities of $Y=1$ associated with each draw of $\beta_{X}$ and $\beta_{Female}$, and the defined values of $X$ and $Female$. Note that we define the scale of $X$ to range between its min and max values, \texttt{length.out=100}. That means, for each simulated $\beta_{X}$ and $\beta_{Female}$, we calculate 100 predicted probability values across the full range of $X$ when $Female$=1, then 100 predicted probability values when $Female$=0. Because we simulated 1,000 draws (1,000 pairs of $\beta_{X}$ and $\beta_{Female}$), we calculate 200,000 values in total. 

\item We then make a data frame including all these simulated probabilities, and put the datafile in long-format. This is the specific data format needed for using \texttt{ggplot2} to graph the simulation results. Finally, we use \texttt{ggplot2} to graph the simulated predicted probabilities (Pr(Y=1)) across the full range of variable $X$, but separating scenarios based on gender. 
\end{enumerate}

```{r}
# Generate predictions
nSims <- 1000
someScenarios <- expand.grid(1,  # Intercept
              seq(min(simdata2$X), max(simdata2$X), len = 100),  
              # A sequence covering the range of X values
              c(0, 1))  # The minimum and maximum of a binary variable

simDraws <- mvrnorm(nSims, coef(myModel), vcov(myModel))
simYhats <- plogis(simDraws %*% t(someScenarios))
            #t():matrix transpose function
dim(simYhats)  #Simulated predictions

# Combine scenario definitions(gender id) and predictions.
predictionFrame <- data.frame(someScenarios, t(simYhats))
# Reshape wide -> long (for ggplot2)
longFrame <- melt(predictionFrame, id.vars = colnames(someScenarios))
head(longFrame)
```


```{r, fig.height=4, fig.width=4}
p1<-ggplot(data = longFrame,
      aes(x = Var2, y = value, group = paste(variable, Var3), 
      colour = factor(Var3)))+
      geom_line(alpha = I(1/sqrt(nSims)))+
      scale_x_continuous("Explanatory Variable X", expand = c(0, 0))+
      scale_y_continuous("Predicted Pr(Y=1)", limits = c(0, 1), expand = c(0, 0))+
      scale_colour_brewer(palette="Set1", labels = c("Male", "Female"))+  
      guides(colour = guide_legend("Group ID", override.aes = list(alpha = 1)))+  
      # Avoid an alpha-related legend problem
      ggtitle("The Effect of X on Y by Gender Group")+
      theme_bw()
print(p1)  # This might take a few seconds...
```



